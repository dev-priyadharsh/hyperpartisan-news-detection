{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch transformers scikit-learn pandas matplotlib seaborn lxml tqdm\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "def parse_articles(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    rows = []\n",
    "    for article in root.findall('article'):\n",
    "        article_id = article.attrib.get('id')\n",
    "        title = article.attrib.get('title', '')\n",
    "        published_at = article.attrib.get('published-at', '')\n",
    "        text = ' '.join([elem.text.strip() for elem in article.iter() if elem.text and elem is not article])\n",
    "        rows.append({'id': article_id, 'title': title, 'published_at': published_at, 'text': text.strip()})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def parse_labels(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    rows = []\n",
    "    for article in root.findall('article'):\n",
    "        label = 1 if article.attrib.get('hyperpartisan') == \"true\" else 0\n",
    "        rows.append({'id': article.attrib['id'], 'label': label})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "articles = parse_articles('articles-training-byarticle-20181122.xml')\n",
    "labels = parse_labels('ground-truth-training-byarticle-20181122.xml')\n",
    "df = pd.merge(articles, labels, on='id')\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text_clean'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 256\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_enc = encode_texts(X_train)\n",
    "test_enc  = encode_texts(X_test)\n",
    "\n",
    "y_train_tensor = torch.tensor(list(y_train), dtype=torch.long)\n",
    "y_test_tensor  = torch.tensor(list(y_test), dtype=torch.long)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_enc, list(y_train))\n",
    "test_dataset  = NewsDataset(test_enc,  list(y_test))\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    return {\n",
    "        'accuracy': (preds == labels).mean(),\n",
    "        'roc_auc': roc_auc_score(labels, pred.predictions[:,1])\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "outputs = trainer.predict(test_dataset)\n",
    "y_true = outputs.label_ids\n",
    "y_scores = outputs.predictions[:, 1]\n",
    "y_pred = np.argmax(outputs.predictions, axis=-1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
